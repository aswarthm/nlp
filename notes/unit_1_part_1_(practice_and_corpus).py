# -*- coding: utf-8 -*-
"""Unit 1 - Part 1 (practice and Corpus).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_bsR1bTI1fu5hHzfpyjG9BNkKPwDAxkg
"""

#For introduction to python, start with https://www.nltk.org/book/ch01.html. Lets see some

import nltk

monty = "Monty Python's "\      #Lets do some string experiment
"Flying Circus."

monty*2 + "plus just last word:" + monty[-7:]     #-7 means negative count from end +7 is from beginning

monty.upper() +' and '+ monty.lower()

monty.replace('y','z')

#Lists - As opposed to strings lists are flexible about the elements they contain
sent1 = ['Monty','Python']
sent2 = ['and','the','holy','grail']
len(sent2)

sent1[1]

sent2.append("1975")
sent1+sent2

sorted(sent1+sent2)

''.join(['Monty','Python'])

'Monty Python'.split()

from nltk.book import text4
text4.concordance("vote")    #displays the context of work "vote" in textbook text4

text4.similar("vote")  #Find words that appear in similar context

nltk.download('stopwords')
text4.collocations()  #words that appear together frequently

len(text4)   #count the vocabulary

len(set(text4))  #distinct words

len(text4)/len(set(text4))   #Richness of text

len(set([word.lower() for word in text4 if len(word)>5]))   #lowercase and length >5

[w.upper() for w in text4[0:5]]    #first five words in text

for word in text4[0:5]:    #first five words
    if len(word)<5 and word.endswith('e'):
        print(word, 'is short and ends with e')
    elif word.istitle():
        print(word, 'is title case')
    else:
        print(word, 'is just another word')

from nltk.corpus import brown
>>> brown.categories()

brown.words(categories='news')

brown.words(fileids=['cg22'])

brown.sents(categories=['news', 'editorial', 'reviews'])

from nltk.corpus import brown                        #print frequency of modal words
>>> news_text = brown.words(categories='news')
>>> fdist = nltk.FreqDist(w.lower() for w in news_text)
>>> modals = ['can', 'could', 'may', 'might', 'must', 'will']
>>> for m in modals:
...     print(m + ':', fdist[m], end=' ')

cfd = nltk.ConditionalFreqDist(                #Print frequency distribution of modal words
...           (genre, word)
...           for genre in brown.categories()
...           for word in brown.words(categories=genre))
>>> genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']
>>> modals = ['can', 'could', 'may', 'might', 'must', 'will']
>>> cfd.tabulate(conditions=genres, samples=modals)

nltk.download('indian')
nltk.corpus.indian.words('hindi.pos')            #Indian corpus

raw = gutenberg.raw("burgess-busterbrown.txt")     #prints first 20 characcters
raw[1:20]

words = gutenberg.words("burgess-busterbrown.txt")   #prints first 20 words
>>> words[1:20]

sents = gutenberg.sents("burgess-busterbrown.txt") #first 20 sentences
>>> sents[1:20]

from nltk.corpus import PlaintextCorpusReader
>>> corpus_root = 'C:\Corpus'   # Root folder where the text files are located
filelists = PlaintextCorpusReader(corpus_root, '.*')  # Read the list of files
filelists.fileids()  # List down the IDs of the files read from the local storage
wordslist = filelists.words('words') # Read the text from specific file  . similarly use sents paras
wordslist

filtered_words = [words for words in set(wordslist) if len(words) > 3]
filtered_words

from nltk.probability import FreqDist
#
# Frequency distribution
#
fdist = FreqDist(wordslist)
#
# Plot the frequency distribution of 30 words with
# cumulative = True
#
fdist.plot(30, cumulative=True)

from nltk.corpus import brown
>>> cfd = nltk.ConditionalFreqDist(
...           (genre, word)
...           for genre in brown.categories()
...           for word in brown.words(categories=genre))
#two genres, news and romance. For each genre [2], we loop over every word in the genre [3], producing pairs consisting of the genre and the word [1]:
genre_word = [(genre, word) [1]
...               for genre in ['news', 'romance']
...               for word in brown.words(categories=genre)]
>>> len(genre_word)

genre_word[:4]   #start 4 words

genre_word[-4:]  #last 4 words

cfd = nltk.ConditionalFreqDist(genre_word)
cfd

print(cfd['news'])

print(cfd['romance'])

cfd['romance'].most_common(20)

from nltk.corpus import stopwords   #stopwords - high-frequency words like the, to and also that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts.
>>> stopwords.words('english')

def content_fraction(text):      # a function to compute what fraction of words in a text are not in the stopwords list:
...     stopwords = nltk.corpus.stopwords.words('english')
...     content = [w for w in text if w.lower() not in stopwords]
...     return len(content) / len(text)
...
content_fraction(nltk.corpus.brown.words())

nltk.download('names')
names = nltk.corpus.names   #The male and female names are stored in separate files. Let's find names which appear in both files,
names.fileids()
['female.txt', 'male.txt']
male_names = names.words('male.txt')
female_names = names.words('female.txt')
[w for w in male_names if w in female_names]

cfd = nltk.ConditionalFreqDist(
...           (fileid, name[-1])  #last letter of name
...           for fileid in names.fileids()
...           for name in names.words(fileid))
>>> cfd.plot()

#wordnet- large lexical database of English https://wordnet.princeton.edu/ with synonyms, antonyms, hyponims, entailment etc.
nltk.download('wordnet')
from nltk.corpus import wordnet as wn
wn.synsets('motorcar')   #motorcar has just one possible meaning and it is identified as car.n.01, the first noun sense of car. The entity car.n.01 is called a synset, or "synonym set", a collection of synonymous words (or "lemmas")

wn.synset('car.n.01').lemma_names()  #other lemmas of car

wn.synset('car.n.01').definition()  #gives definition of the word

wn.synset('car.n.01').examples()  #gives example usage



wn.synset('car.n.01').lemmas()  #all the lemmas for a given synset

wn.lemma('car.n.01.automobile') #look up a particular lemma

wn.lemma('car.n.01.automobile').synset() #get the synset corresponding to a lemma

wn.lemma('car.n.01.automobile').name() #get the "name" of a lemma

wn.synsets('car') #Unlike the word motorcar, which is unambiguous and has one synset, the word car is ambiguous, having five synsets:

for synset in wn.synsets('car'):
    print(synset.lemma_names())

wn.lemmas('car')

motorcar = wn.synset('car.n.01')
>>> types_of_motorcar = motorcar.hyponyms()   #a word of more specific meaning than a general or superordinate term applicable to it. For example, spoon is a hyponym of cutlery.
>>> types_of_motorcar[0]

sorted(lemma.name() for synset in types_of_motorcar for lemma in synset.lemmas())

