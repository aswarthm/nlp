# -*- coding: utf-8 -*-
"""N-Gram Tagger.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QyqMimA76REnp9XgKjK5rarvIsi3U3XG
"""

from nltk import ngrams  #n-grams are a contiguous sequence of n items from a given sample of text or speech. This is unigram
sentence = 'this is a foo bar sentences and I want to ngramize it'
n = 1
sixgrams = ngrams(sentence.split(), n)
for grams in sixgrams:
  print(grams)

from nltk import ngrams   #This is bigram
sentence = 'this is a foo bar sentences and I want to ngramize it'
n = 2
sixgrams = ngrams(sentence.split(), n)
for grams in sixgrams:
  print(grams)

from nltk.corpus import brown   #inport brown corpus

import nltk

# Fiction    - Unigram tagger
brown_tagged_sents = brown.tagged_sents(categories='fiction')[:500]  #train with last 500 sentences
unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)
unigram_tagger.evaluate(brown.tagged_sents(categories='fiction')[501:600])  #evaluate with 500-600 sentences

brown_tagged_sents = brown.tagged_sents(categories='fiction')[:500]
print(brown_tagged_sents)

from nltk.corpus import brown
>>> brown_tagged_sents = brown.tagged_sents(categories='news')
>>> brown_sents = brown.sents(categories='news')
>>> unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)
>>> unigram_tagger.tag(brown_sents[2009])

unigram_tagger.evaluate(brown_tagged_sents)

size = int(len(brown_tagged_sents) * 0.9)   #split the sentences 90% training, 10% testing
size

train_sents = brown_tagged_sents[:size]  #train on the last 4160 sentences
>>> test_sents = brown_tagged_sents[size:]   #test on the rest of the sentences
>>> unigram_tagger = nltk.UnigramTagger(train_sents)
>>> unigram_tagger.evaluate(test_sents)

bigram_tagger = nltk.BigramTagger(train_sents)   #bigram tagger (n-1) ie one word before unseen_sent = brown_sents[4203]
brown_sents = brown.sents(categories='news')
bigram_tagger.tag(brown_sents[2007])
unseen_sent = brown_sents[4203]
bigram_tagger.tag(unseen_sent)



bigram_tagger.evaluate(test_sents)  #As n gets larger, the specificity of the contexts increases, as does the chance that the data we wish to tag contains contexts that were not present in the training data. This is known as the sparse data problem, and is quite pervasive in NLP. As a consequence, there is a trade-off between the accuracy and the coverage of our results (and this is related to the precision/recall trade-off in information retrieval).

#combining taggers
t0 = nltk.DefaultTagger('NN')
t1 = nltk.UnigramTagger(train_sents, backoff=t0)
t2 = nltk.BigramTagger(train_sents, backoff=t1)
t2.evaluate(test_sents)   #Try tagging the token with the bigram tagger. If the bigram tagger is unable to find a tag for the token, try the unigram tagger. If the unigram tagger is also unable to find a tag, use a default tagger.

from pickle import dump     #Instead of training a tagger every time we need one, it is convenient to save a trained tagger in a file for later re-use. Let's save our tagger t2 to a file t2.pkl.
>>> output = open('t2.pkl', 'wb')
>>> dump(t2, output, -1)
>>> output.close()

from pickle import load
>>> input = open('t2.pkl', 'rb')
>>> tagger = load(input)
>>> input.close()

text = """The board's action shows what free enterprise
...     is up against in our complex maze of regulatory laws ."""
>>> tokens = text.split()
>>> tagger.tag(tokens)

pwd     #you will fins pkl file here

# Read How to Determine the Category of a Word from https://www.nltk.org/book/ch05.html

