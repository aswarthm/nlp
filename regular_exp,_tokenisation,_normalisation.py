# -*- coding: utf-8 -*-
"""Regular exp, tokenisation, normalisation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R4TvBdHhe9H8CxoDTO4Mrf52BCgKaHf6
"""

import nltk
import re
wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]
[w for w in wordlist if re.search('ed$', w)]    #list words ending with ed

[w for w in wordlist if re.search('^..j..t..$', w)]   #^ beginning $ end

[w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)]   #textonyms - Two or more words that are entered with the same sequence of keystrokes

chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))
[w for w in chat_words if re.search('^m+i+n+e+$', w)]      #any number of times

[w for w in chat_words if re.search('^[ha]+$', w)]   #+ one or more occurance

wsj = sorted(set(nltk.corpus.treebank.words()))
>>> [w for w in wsj if re.search('^[0-9]+\.[0-9]+$', w)]

[w for w in wsj if re.search('^[A-Z]+\$$', w)] #escape character for $

[w for w in wsj if re.search('^[0-9]{4}$', w)]  #only 4 digits

[w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)] #minimum 3 and max 5 characters

[w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)]   #minimum 5 in first word and max 6 in last

[w for w in wsj if re.search('(ed|ing)$', w)]

word = 'supercalifragilisticexpialidocious'
>>> re.findall(r'[aeiou]', word)

len(re.findall(r'[aeiou]', word))

wsj = sorted(set(nltk.corpus.treebank.words()))
>>> fd = nltk.FreqDist(vs for word in wsj
...                       for vs in re.findall(r'[aeiou]{2,}', word))
>>> fd.most_common(12)

# The regular expression in our next example matches initial vowel sequences, final vowel sequences, and all consonants; everything else is ignored. This three-way disjunction is processed left-to-right, if one of the three parts matches the word, any later parts of the regular expression are ignored. We use re.findall() to extract all the matching pieces, and ''.join() to join them together
nltk.download('udhr')
regexp = r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]'
def compress(word):
    pieces = re.findall(regexp, word)
    return ''.join(pieces)

english_udhr = nltk.corpus.udhr.words('English-Latin1')
print(nltk.tokenwrap(compress(w) for w in english_udhr[:75]))

rotokas_words = nltk.corpus.brown.words(categories='news')
>>> cvs = [cv for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]
>>> cfd = nltk.ConditionalFreqDist(cvs)
>>> cfd.tabulate()

#If we want to be able to inspect the words behind the numbers in the above table, it would be helpful to have an index, allowing us to quickly find the list of words that contains a given consonant-vowel pair, e.g. cv_index['su'] should give us all words containing s
cv_word_pairs = [(cv, w) for w in rotokas_words
for cv in re.findall(r'[ptksvr][aeiou]', w)]
cv_index = nltk.Index(cv_word_pairs)
cv_index['su']

re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing') #Only the one in bracket is printed. scope of disjunction
['ing']

re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')  #?: prints everything

re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing') #split the word

re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes') #matches s instead of es, * is greedy

re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes') #*? is non-greedy

#This works even when we allow an empty suffix, by making the content of the second parentheses optional
re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$', 'language')

#complete stemmer
from nltk.tokenize import word_tokenize

def stem(word):
    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'
    stem, suffix = re.findall(regexp, word)[0]
    return stem

raw = """DENNIS: Listen, strange women lying in ponds distributing swords is no basis for a system of government.  Supreme executive power derives from
a mandate from the masses, not from some farcical aquatic ceremony."""
tokens = word_tokenize(raw)
[stem(t) for t in tokens]

from nltk.corpus import gutenberg, nps_chat    #matches "a monied man" and prints only in bracket monied
>>> moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))
>>> moby.findall(r"<a> (<.*>) <man>")

chat = nltk.Text(nps_chat.words())
>>> chat.findall(r"<.*> <.*> <bro>")

chat.findall(r"<l.*>{3,}")

from nltk.corpus import brown
>>> hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))
>>> hobbies_learned.findall(r"<\w*> <and> <other> <\w*s>")   #\w is alphaneumeric followed by and other <word> ends with s

porter = nltk.PorterStemmer()
>>> lancaster = nltk.LancasterStemmer()
>>> [porter.stem(t) for t in tokens]  #Apply porter stemmer

[lancaster.stem(t) for t in tokens]   #Apply lancaster stemmer, observe lying - porter is better

wnl = nltk.WordNetLemmatizer() #The WordNet lemmatizer only removes affixes if the resulting word is in its dictionary
>>> [wnl.lemmatize(t) for t in tokens]

#tokenisation
raw = """'When I'M a Duchess,' she said to herself, (not in a very hopeful tone
... though), 'I won't have any pepper in my kitchen AT ALL. Soup does very
... well without--Maybe it's always pepper that makes people hot-tempered,'..."""
re.split(r' ', raw)   #split based on white spacce -- not correct observe \n

re.split(r'[ \t\n]+', raw)  #problem here is , ( )

re.split(r'\W+', raw)  # \W is non alphaneumeric. 's missing

re.findall(r'\w+|\S\w*', raw)  #\S is any non white character

print(re.findall(r"\w+(?:[-']\w+)*|'|[-.(]+|\S\w*", raw)) #matches - in hot tempered, 's

#proper tokenizer
text = 'That U.S.A. poster-print costs $12.40...'
pattern = r'''(?x)     # set flag to allow verbose regexps
    (?:[A-Z]\.)+       # abbreviations, e.g. U.S.A.
    | \w+(?:-\w+)*       # words with optional internal hyphens
    | \$?\d+(?:\.\d+)?%? # currency and percentages, e.g. $12.40, 82%
    | \.\.\.             # ellipsis
    | [][.,;"'?():-_`]   # these are separate tokens; includes ], [
    '''
parsed = re.findall(pattern, text)
print(parsed)

