{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "516027f0",
      "metadata": {
        "scrolled": true,
        "id": "516027f0",
        "outputId": "ebadc007-b774-48fb-da34-dfa4e72cd2c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('this',)\n",
            "('is',)\n",
            "('a',)\n",
            "('foo',)\n",
            "('bar',)\n",
            "('sentences',)\n",
            "('and',)\n",
            "('I',)\n",
            "('want',)\n",
            "('to',)\n",
            "('ngramize',)\n",
            "('it',)\n"
          ]
        }
      ],
      "source": [
        "from nltk import ngrams  #n-grams are a contiguous sequence of n items from a given sample of text or speech. This is unigram\n",
        "sentence = 'this is a foo bar sentences and I want to ngramize it'\n",
        "n = 1\n",
        "sixgrams = ngrams(sentence.split(), n)\n",
        "for grams in sixgrams:\n",
        "  print(grams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d904438",
      "metadata": {
        "id": "2d904438",
        "outputId": "6aecf09b-ef51-496d-9706-541637da1aea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('this', 'is')\n",
            "('is', 'a')\n",
            "('a', 'foo')\n",
            "('foo', 'bar')\n",
            "('bar', 'sentences')\n",
            "('sentences', 'and')\n",
            "('and', 'I')\n",
            "('I', 'want')\n",
            "('want', 'to')\n",
            "('to', 'ngramize')\n",
            "('ngramize', 'it')\n"
          ]
        }
      ],
      "source": [
        "from nltk import ngrams   #This is bigram\n",
        "sentence = 'this is a foo bar sentences and I want to ngramize it'\n",
        "n = 2\n",
        "sixgrams = ngrams(sentence.split(), n)\n",
        "for grams in sixgrams:\n",
        "  print(grams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca58948e",
      "metadata": {
        "id": "ca58948e"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import brown   #inport brown corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe015c53",
      "metadata": {
        "id": "fe015c53"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aee4fd51",
      "metadata": {
        "id": "aee4fd51",
        "outputId": "eae519e7-746b-4963-a9c3-b06993e06ddd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\deepa\\AppData\\Local\\Temp\\ipykernel_20888\\3023479701.py:4: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  unigram_tagger.evaluate(brown.tagged_sents(categories='fiction')[501:600])  #evaluate with 500-600 sentences\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.7474610697359513"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fiction    - Unigram tagger\n",
        "brown_tagged_sents = brown.tagged_sents(categories='fiction')[:500]  #train with last 500 sentences\n",
        "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
        "unigram_tagger.evaluate(brown.tagged_sents(categories='fiction')[501:600])  #evaluate with 500-600 sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e95540c",
      "metadata": {
        "id": "9e95540c",
        "outputId": "bd277527-1485-4415-8a2e-e8c102470313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[('Thirty-three', 'CD-HL')], [('Scotty', 'NP'), ('did', 'DOD'), ('not', '*'), ('go', 'VB'), ('back', 'RB'), ('to', 'IN'), ('school', 'NN'), ('.', '.')], ...]\n"
          ]
        }
      ],
      "source": [
        "brown_tagged_sents = brown.tagged_sents(categories='fiction')[:500]\n",
        "print(brown_tagged_sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18aa68d0",
      "metadata": {
        "id": "18aa68d0",
        "outputId": "b18084f3-f863-4677-b8b9-dae8766403b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('The', 'AT'),\n",
              " ('structures', 'NNS'),\n",
              " ('housing', 'VBG'),\n",
              " ('the', 'AT'),\n",
              " ('apartments', 'NNS'),\n",
              " ('are', 'BER'),\n",
              " ('of', 'IN'),\n",
              " ('masonry', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('frame', 'NN'),\n",
              " ('construction', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.corpus import brown\n",
        ">>> brown_tagged_sents = brown.tagged_sents(categories='news')\n",
        ">>> brown_sents = brown.sents(categories='news')\n",
        ">>> unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
        ">>> unigram_tagger.tag(brown_sents[2009])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0d60fff",
      "metadata": {
        "id": "d0d60fff",
        "outputId": "cb6f0967-c546-4bae-c8ad-42c4fd997166"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\deepa\\AppData\\Local\\Temp\\ipykernel_23624\\1449911821.py:1: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  unigram_tagger.evaluate(brown_tagged_sents)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9349006503968017"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unigram_tagger.evaluate(brown_tagged_sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3452e486",
      "metadata": {
        "id": "3452e486",
        "outputId": "2e7d697d-1e37-48af-9f9e-b683a21fbdb1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "450"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "size = int(len(brown_tagged_sents) * 0.9)   #split the sentences 90% training, 10% testing\n",
        "size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10158870",
      "metadata": {
        "id": "10158870",
        "outputId": "e2f87b4a-5d72-4b4e-d393-fab11d78e646"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\deepa\\AppData\\Local\\Temp\\ipykernel_20888\\1267303945.py:4: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  unigram_tagger.evaluate(test_sents)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.6773255813953488"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_sents = brown_tagged_sents[:size]  #train on the last 4160 sentences\n",
        ">>> test_sents = brown_tagged_sents[size:]   #test on the rest of the sentences\n",
        ">>> unigram_tagger = nltk.UnigramTagger(train_sents)\n",
        ">>> unigram_tagger.evaluate(test_sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1fe6cc4",
      "metadata": {
        "id": "c1fe6cc4",
        "outputId": "1d5231d2-f899-4943-990d-e38c886382ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('The', 'AT'),\n",
              " ('population', None),\n",
              " ('of', None),\n",
              " ('the', None),\n",
              " ('Congo', None),\n",
              " ('is', None),\n",
              " ('13.5', None),\n",
              " ('million', None),\n",
              " (',', None),\n",
              " ('divided', None),\n",
              " ('into', None),\n",
              " ('at', None),\n",
              " ('least', None),\n",
              " ('seven', None),\n",
              " ('major', None),\n",
              " ('``', None),\n",
              " ('culture', None),\n",
              " ('clusters', None),\n",
              " (\"''\", None),\n",
              " ('and', None),\n",
              " ('innumerable', None),\n",
              " ('tribes', None),\n",
              " ('speaking', None),\n",
              " ('400', None),\n",
              " ('separate', None),\n",
              " ('dialects', None),\n",
              " ('.', None)]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_sents)   #bigram tagger (n-1) ie one word before unseen_sent = brown_sents[4203]\n",
        "brown_sents = brown.sents(categories='news')\n",
        "bigram_tagger.tag(brown_sents[2007])\n",
        "unseen_sent = brown_sents[4203]\n",
        "bigram_tagger.tag(unseen_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c69ebde6",
      "metadata": {
        "id": "c69ebde6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63fb2816",
      "metadata": {
        "id": "63fb2816",
        "outputId": "5c3b0b34-14a0-4a80-8ded-2a92a931e0f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\deepa\\AppData\\Local\\Temp\\ipykernel_20888\\1969279728.py:1: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  bigram_tagger.evaluate(test_sents)  #As n gets larger, the specificity of the contexts increases, as does the chance that the data we wish to tag contains contexts that were not present in the training data. This is known as the sparse data problem, and is quite pervasive in NLP. As a consequence, there is a trade-off between the accuracy and the coverage of our results (and this is related to the precision/recall trade-off in information retrieval).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.07364341085271318"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigram_tagger.evaluate(test_sents)  #As n gets larger, the specificity of the contexts increases, as does the chance that the data we wish to tag contains contexts that were not present in the training data. This is known as the sparse data problem, and is quite pervasive in NLP. As a consequence, there is a trade-off between the accuracy and the coverage of our results (and this is related to the precision/recall trade-off in information retrieval)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09a3a23f",
      "metadata": {
        "id": "09a3a23f",
        "outputId": "71e49b77-1d1f-4d58-e829-a147c9f66727"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\deepa\\AppData\\Local\\Temp\\ipykernel_20888\\4246112177.py:5: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  t2.evaluate(test_sents)   #Try tagging the token with the bigram tagger. If the bigram tagger is unable to find a tag for the token, try the unigram tagger. If the unigram tagger is also unable to find a tag, use a default tagger.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.7751937984496124"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#combining taggers\n",
        "t0 = nltk.DefaultTagger('NN')\n",
        "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
        "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
        "t2.evaluate(test_sents)   #Try tagging the token with the bigram tagger. If the bigram tagger is unable to find a tag for the token, try the unigram tagger. If the unigram tagger is also unable to find a tag, use a default tagger."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caee7c62",
      "metadata": {
        "id": "caee7c62"
      },
      "outputs": [],
      "source": [
        "from pickle import dump     #Instead of training a tagger every time we need one, it is convenient to save a trained tagger in a file for later re-use. Let's save our tagger t2 to a file t2.pkl.\n",
        ">>> output = open('t2.pkl', 'wb')\n",
        ">>> dump(t2, output, -1)\n",
        ">>> output.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed08726e",
      "metadata": {
        "id": "ed08726e"
      },
      "outputs": [],
      "source": [
        "from pickle import load\n",
        ">>> input = open('t2.pkl', 'rb')\n",
        ">>> tagger = load(input)\n",
        ">>> input.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbf8015c",
      "metadata": {
        "id": "fbf8015c",
        "outputId": "fc197bf9-0d5e-4df7-c476-7a9ff1cf8cbc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('The', 'AT'),\n",
              " (\"board's\", 'NN'),\n",
              " ('action', 'NN'),\n",
              " ('shows', 'NN'),\n",
              " ('what', 'WDT'),\n",
              " ('free', 'NN'),\n",
              " ('enterprise', 'NN'),\n",
              " ('is', 'BEZ'),\n",
              " ('up', 'RP'),\n",
              " ('against', 'IN'),\n",
              " ('in', 'IN'),\n",
              " ('our', 'PP$'),\n",
              " ('complex', 'NN'),\n",
              " ('maze', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('regulatory', 'NN'),\n",
              " ('laws', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"\"\"The board's action shows what free enterprise\n",
        "...     is up against in our complex maze of regulatory laws .\"\"\"\n",
        ">>> tokens = text.split()\n",
        ">>> tagger.tag(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81eddfa0",
      "metadata": {
        "id": "81eddfa0",
        "outputId": "f450a6d3-2193-4971-8f83-bdb96fcfab5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'C:\\\\Users\\\\deepa'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd     #you will fins pkl file here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c26b8765",
      "metadata": {
        "id": "c26b8765"
      },
      "outputs": [],
      "source": [
        "# Read How to Determine the Category of a Word from https://www.nltk.org/book/ch05.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7508bea",
      "metadata": {
        "id": "e7508bea"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}